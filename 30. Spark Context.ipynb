{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ae7363-2247-49cb-b32d-e46ae2866ad6",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "\n",
    "Let's understand about SparkContext and how to start it using SparkSession.\n",
    "\n",
    "* `SparkContext` maintains the context of all the jobs that are submitted until it is killed.\n",
    "\n",
    "* `SparkSession` is a class that is part of `org.apache.spark.sql` package.\n",
    "\n",
    "* `SparkSession` is nothing but wrapper on top of `SparkContext`. `SparkSession` creates `SparkContext` internally.\n",
    "\n",
    "* When Spark application is submitted using `spark-submit` or `spark-shell` or `pyspark`, a web service called as `Spark Context` will get started.\n",
    "\n",
    "* Here is the example about how `Spark Shell` can be launched locally.\n",
    "\n",
    "```\n",
    "spark-shell \\\n",
    "    --master \"local[*]\"\n",
    "```\n",
    "\n",
    "*  Even if we don't specify `master`, By default it will be using `local` mode only. However, we will not be able to say `--master yarn` on the `local` machine unless we have `Hadoop` also setup and integrated with `Spark`.\n",
    "\n",
    "* `Spark Shell` can be launched on `multi-node` cluster But, first we need to understand that which custer mode we are using among `Yarn / Mesos / Standalone`.\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0\n",
    "```\n",
    "\n",
    "* Whenever any SparkContext has been created, there will be web service that will be associated with the port number. By default, it is `4040`, but we can change it as per the availability of the port."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58107fe-d2de-49ac-b803-f353d2fcee9b",
   "metadata": {},
   "source": [
    "### Create SparkSession\n",
    "\n",
    "* We can create `SparkSession` object with any name But typically we use `spark`. Once it is created, several APIs will be exposed including `read`.\n",
    "\n",
    "* We need to atleast set `Application Name` and specify `execution mode` in which SparkContext should run while creating `SparkSession` object. \n",
    "\n",
    "* We can use `appName` to specify name for the application and `master` to specify the execution mode.\n",
    "\n",
    "* `getOrCreate` will use any existing SparkSession if it is available else create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bacee4-bc9b-4db3-b4ab-ddbdf29a2842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.20.10.2:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1670418772804)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 18:42:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f3d51f3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "                        .config(\"spark.ui.port\", \"0\")\n",
    "                        .appName(\"Spark Context\")\n",
    "                        .master(\"yarn\")\n",
    "                        .getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80a7d2-2f28-4e37-b5f9-3377f9537682",
   "metadata": {},
   "source": [
    "* There are two different modes of `yarn` as `yarn-client` or `yarn-cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4998fdc0-90d0-4f15-bb6c-8d91f1be22cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f3d51f3\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b60a-c7a9-454b-b58c-295352451ba7",
   "metadata": {},
   "source": [
    "* To get the details related to SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e14a74-b7a1-4d03-89b3-71a9e8ca0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.extraJavaOptions,-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED)\n",
      "(spark.app.submitTime,1670418772058)\n",
      "(spark.driver.port,50805)\n",
      "(spark.app.startTime,1670418772161)\n",
      "(spark.executor.id,driver)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.driver.host,172.20.10.2)\n",
      "(spark.rdd.compress,True)\n",
      "(spark.executor.extraJavaOptions,-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED)\n",
      "(spark.app.id,local-1670418772804)\n",
      "(spark.sql.warehouse.dir,file:/Users/suryakantkumar/Documents/Apache%20Spark%20with%20Scala/spark-warehouse)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.pyFiles,)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.repl.class.uri,spark://172.20.10.2:50805/classes)\n",
      "(spark.ui.showConsoleProgress,true)\n",
      "(spark.repl.class.outputDir,/var/folders/c7/nttrfbrd6v7bxx4pz6wxkzqm0000gn/T/tmp2vyodqwy)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
